## Python Web Software Developer
- Designed and implemented high-performance RESTful APIs using Python and Flask framework on Azure Cloud, ensuring efficient handling of financial data transactions.
- Developed JSON-based services adhering to JSON Schema specification for precise validation and structuring of financial data payloads, ensuring data integrity and compliance.
- Architected and hosted scalable and reliable backend services using Flask within Azure App Service, employing Flask servers to develop and deploy RESTful APIs and backend logic for seamless integration into financial applications.
- Created comprehensive test cases using tools like pytest and unittest to validate functionality, performance, and security aspects of the developed Flask APIs and frontend interfaces.
- Established a robust DevOps pipeline using Azure Pipelines to automate build, test, and deployment processes of Flask applications, ensuring continuous integration and delivery in financial software projects.
- Utilized Postman for rigorous API testing, including functional, integration, and performance testing, to validate endpoints handling financial transactions securely and efficiently.
- Designed and implemented interactive web interfaces using HTML, CSS, JavaScript, and jQuery, optimizing user interaction and visualization of financial data dashboards and reporting tools.
- Implemented OAuth 2.0 authentication and role-based access control (RBAC) mechanisms within Flask APIs hosted on Azure, ensuring secure access and protection of financial data.
- Managed application databases on Azure SQL Database, implementing schema design and query optimization techniques to support high-volume transactional data processing in financial applications.
- Integrated Azure Redis Cache for implementing caching strategies, significantly improving API response times and scalability for real-time financial data retrieval and processing.
- Collaborated with cross-functional teams to design and implement microservices architecture patterns in the Azure DevOps pipeline, ensuring modular scalability and maintainability of financial data applications using CI/CD.
- Documented technical specifications and architectural diagrams using OpenAPI specification, facilitating clear communication and alignment across development teams for seamless integration and maintenance of financial software solutions.


## Compendious Medialabs Pvt. Ltd. (Mumbai, IN) | Python Engineer (Intern)
- Developed Python scripts to automate tasks using APIs, including creation and utilization, and implemented web scraping and testing with Selenium.
- Utilized Python libraries such as requests for efficient handling of HTTP requests for data collection.
- Used libraries like Pandas, and NumPy for sorting and filtering data and used Databases for storing, and retrieving data.
- Leveraged Redis for caching and optimizing data retrieval processes, enhancing system performance and efficiency.
- Integrated RSS feed handling into Python scripts, enabling streamlined data processing and utilization for various applications.
- Utilized Azure services for virtual machines, databases, and other infrastructure requirements, ensuring scalable and reliable automation solutions.
- Implemented end-to-end automation workflows, ensuring seamless execution of tasks from data acquisition to storage and processing.
- Contributed to the development of internal tools to streamline repetitive tasks, increasing team productivity and efficiency.



## AZURE DATA ARCHITECT JD
Design & Develop the ETL
Good experience in writing SQL, Python and Pyspark programming
Create the Pipelines (simple and complex) using ADF.
Work with other Azure stack modules like Azure Data Lakes, SQL DW
Must be extremely well versed with handling large volume of data.
Understand the business requirements for Data flow process needs.
Understand requirements, functional and technical specification documents.
Development of mapping document and transformation business rules as per scope and requirements/Source to target.
Responsible for continuous formal and informal communication on project status
Good understanding of JIRA stories process for SQL development activities
Overall, 4+ years of developer skills with SQL, Python with Spark (Pyspark)
Experience in Azure Data Factory, Data Sets, Data Frame, Azure Blob & Storage Explorer
Implement data ingestion pipelines from multiple data sources using ADF, ADB (Azure data bricks)
Experience in creating Data Factory Pipelines, custom Azure development, deployment, troubleshoot data load / extraction using ADF.
Extensive experience on SQL, python, Pyspark in Azure databricks.
Able to write Python code in Pyspark frame by using Dataframes.
Have good understanding on Agile/Scrum methodologies.


## PYTHON DATA DISEP
https://jobs.disneycareers.com/job/-/-/391/66668296800?p_sid=dRKsD8b&p_uid=Nw45aK3c6x&ss=paid&utm_campaign=deetech_coe_us&utm_content=pj_board&utm_medium=jobad&utm_source=dice&dclid=CPuU8LeurocDFRuwOgUdZWA46g


## Python LLM Cloud JD
Advanced Python programming
Large-scale implementation of Large Language Models (LLMs) in production environments
Experience in application development and API optimization
Knowledge of databases relevant to LLMs
Strong understanding of LLMs and transformer models
Proficiency with Azure Cloud services, including deployment and management of cloud-based applications
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes)
Experience with CI/CD pipelines and automated testing frameworks
Excellent problem-solving skills and ability to tackle complex challenges
Strong communication and teamwork skills


## Role: Azure Technical Architect
Azure IAAS & PAAS & SAAS • Azure Data Bricks, Data Factory, Pipelines, Event Hubs, Function • apps, Data Engineering • Azure Monitor, Log Analytics Workspaces, Application Insights, • Storage accounts, Dashboards, KQL • Azure Security, Governance and Compliance • Azure solution design and architecture • Foundational SQL experience/usage • Adhere to response and resolution SLAs and escalation processes • in order to ensure fast resolution of customer issues that exceed expectations • Submit well-documented bugs and feature requests arising from customer submitted requests and work with Engineering towards a resolution. • Design and develop frameworks and core functionality. • Identify gaps and come up with working solutions. • Application architecture and solution design in Azure • Analyze production workloads and develop strategies • Utilize technical acumen and in-depth understanding of • business processes and practices to influence the creation, • modification, and execution of operational and strategic plans • Create, maintain, and administer database platforms and applications for company-wide use • Maintain data integrity and database performance, stability,and scalability. Make recommendations to optimize database and application performance and efficiency • Diagnose and resolve database, network, and security issues. 


## PYTHON DEVOPS ENGINEER
Mandate Required skills
Programming: Proficient in Python. Experienced on JavaScript and Node.js.
Web Frameworks: Experience with Flask or Django for web applications and APIs.
API Development: Experience with async framework like Quart or FastAPI for creating fast and efficient APIs.
Azure SDK: Knowledge of Azure SDK for Python for integrating Azure services.
Web authentication and authorization, including LDAP integration with web application
Nice to have:
Familiarity with Requests or HTTPX for interacting with Confluence APIs.
Azure Services: Experience with Azure document AI, Azure AI Search and Azure OpenAI, Azure App Server.


## SENIOR DATA ENGINEER
Requirements
Has demonstrated proficiency in designing and developing Azure Data Factory Pipelines. (2 Years)
Strong Experience designing and implementing Data Warehouse
Experience working with Microsoft BI stack (SSIS/SSRS/SSAS) and Microsoft SQL server. (5 Years)
Must have Experience with at least one Columnar MPP Cloud data warehouse (Snowflake /Azure Synapse / Redshift) (2+ years)
Working knowledge managing data in the Data Lake
Experience with Git and Azure Devops
Experience in Agile, Jira and Confluence
Solid understanding of programming SQL objects (procedures, triggers, views, functions) in SQL Server. Experience optimizing SQL queries a plus
Working Knowledge of Azure Architecture, Data Lake
Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc
Responsibilities
Design, develop and implement scalable batch/real time data pipelines (ETLs) to integrate data from a variety of sources into Data Warehouse and Data Lake
Design and implement data model changes that align with warehouse dimensional modeling standards
Proficient in Data Lake, Data Warehouse Concepts and Dimensional Data Model
Responsible for maintenance and support of all database environments, design and develop data pipelines, workflow, ETL solutions on both on-prem and cloud-based environments
Design and develop SQL stored procedures, functions, views, and triggers
Design, code, test, document and troubleshoot deliverables
Collaborate with others to test and resolve issues with deliverables
Maintain awareness of and ensure adherence to company standards regarding privacy
Create and maintain Design documents, Source to Target mappings, unit test cases, data seeding
Ability to perform Data Analysis and Data Quality tests and create audit for the ETLs
Perform Continuous Integration and deployment using Azure Devops and Git